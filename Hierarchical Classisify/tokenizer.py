import jieba
import json

class Tokenizer:
    def __init__(self, vocab_file, special_map=None):
        pass

    def _get_stop_words_file(self):
        pass

